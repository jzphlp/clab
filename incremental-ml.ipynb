{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jzphlp/clab/blob/dev-clab/incremental-ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVxt0wsexsnh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, median_absolute_error\n",
        "from pprint import pprint\n",
        "import lightgbm as lgb\n",
        "from params_lgb import *\n",
        "from df_utils import *\n",
        "\n",
        "# make this into a function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqabjWjsxsnk",
        "outputId": "ddad2a48-0403-4eab-aa9b-be97698e7905"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "78"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_depth=7\n",
        "num_leaves = (2 ** max_depth) - 50\n",
        "num_leaves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EohK13P4xsnl"
      },
      "outputs": [],
      "source": [
        "########################################################################################\n",
        "# INPUTS\n",
        "########################################################################################\n",
        "save_dir = r\"zdir\"\n",
        "parquets = r\"parquets\"\n",
        "tcolx = 'pdem'\n",
        "fcolx = ['cdem_dem','edem_dem','egm08','tdem_dem']\n",
        "interval = 100#500#4500#\n",
        "\n",
        "modelpath_i1 = os.path.join(save_dir,'model_i1.txt')\n",
        "modelpath_i1_best = os.path.join(save_dir,'model_i1_best.txt')\n",
        "np.random.seed(seed=seed)\n",
        "\n",
        "params = params1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCxy4XTlxsnm",
        "outputId": "8d0f9b45-3968-43f7-f440-53d825ec2f12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1049\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "         R2      RSME       MAE  MedianAE      Bias\n",
            "0  0.570582  0.995628  0.713177   0.51097  0.020527\n",
            "Icrementtal at looop 1/11\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Icrementtal at looop 2/11\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Icrementtal at looop 3/11\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Icrementtal at looop 4/11\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Icrementtal at looop 5/11\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Icrementtal at looop 6/11\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Icrementtal at looop 7/11\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Icrementtal at looop 8/11\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Icrementtal at looop 9/11\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Icrementtal at looop 10/11\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "         R2      RSME       MAE  MedianAE      Bias\n",
            "0  0.570582  0.995628  0.713177   0.51097  0.020527\n",
            "         R2      RSME       MAE  MedianAE     Bias\n",
            "0  0.655635  0.891174  0.640536  0.469108  0.08141\n"
          ]
        }
      ],
      "source": [
        "########################################################################################\n",
        "# MODELLING\n",
        "########################################################################################\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "filenames = glob.glob(f'{parquets}/*.parquet'); print(len(filenames))\n",
        "filenames_shuffled_all = list(shuffle(filenames,random_state=seed))\n",
        "\n",
        "loops = math.ceil(len(filenames_shuffled_all)/interval)\n",
        "filenames_shuffled = filenames_shuffled_all[:interval]\n",
        "train_filenames, val_filenames = train_test_split(\n",
        "    filenames_shuffled,test_size=0.3, random_state=seed)\n",
        "out_val = val_filenames\n",
        "\n",
        "\n",
        "ddf_train = pd.read_parquet(train_filenames)\n",
        "ddf_train = filter_values(ddf_train, tcolx, -20, 1000, -9999)\n",
        "xtrain, ytrain = ddf_train.drop(tcolx, axis=1), ddf_train[tcolx]\n",
        "del ddf_train\n",
        "\n",
        "ddf_valid = pd.read_parquet(val_filenames)\n",
        "ddf_valid = filter_values(ddf_valid, tcolx, -20, 1000, -9999)\n",
        "xvalid, yvalid = ddf_valid.drop(tcolx, axis=1), ddf_valid[tcolx]\n",
        "del ddf_valid\n",
        " # is there a way of spling x,y without having to load the data and then delete\n",
        "train_ds = lgb.Dataset(data=xtrain,label=ytrain,feature_name=fcolx)\n",
        "valid_ds = lgb.Dataset(data=xvalid,label=yvalid,feature_name=fcolx)\n",
        "\n",
        "gbm = lgb.train(params,\n",
        "                train_ds,\n",
        "                num_round,\n",
        "                valid_sets=valid_ds,\n",
        "                feval=mean_error,\n",
        "                keep_training_booster=True, # lear what it does\n",
        "                callbacks=[lgb.early_stopping(stopping_rounds=50)])\n",
        "\n",
        "gbm.save_model(modelpath_i1)\n",
        "gbm.save_model(modelpath_i1_best,num_iteration=gbm.best_iteration)\n",
        "\n",
        "\n",
        "pvalid = gbm.predict(xvalid)\n",
        "derror = Rmetrics(yvalid, pvalid)\n",
        "derror.to_csv(os.path.join(save_dir,'metrics_i1.csv'), index=False)\n",
        "pprint(derror)\n",
        "\n",
        "booster = None\n",
        "for i in range(1,loops):\n",
        "    print(f'Icrementtal at looop {i}/{loops}')\n",
        "    start_index = interval*i\n",
        "    if i == loops - 1:\n",
        "        end_index = len(filenames_shuffled_all) - 1\n",
        "    else:\n",
        "        end_index = interval*i+interval\n",
        "    filenames_shuffled = filenames_shuffled_all[start_index:end_index]\n",
        "    train_filenames, val_filenames = train_test_split(\n",
        "        filenames_shuffled,test_size=0.3, random_state=seed)\n",
        "    out_val = np.append(out_val,val_filenames)\n",
        "\n",
        "    ddf_train = pd.read_parquet(train_filenames)\n",
        "    ddf_train = filter_values(ddf_train, tcolx, -20, 1000, -9999)\n",
        "    xtrain, ytrain = ddf_train.drop(tcolx, axis=1), ddf_train[tcolx]\n",
        "    del ddf_train\n",
        "\n",
        "    ddf_valid = pd.read_parquet(val_filenames)\n",
        "    ddf_valid = filter_values(ddf_valid, tcolx, -20, 1000, -9999)\n",
        "    xvalid, yvalid = ddf_valid.drop(tcolx, axis=1), ddf_valid[tcolx]\n",
        "    del ddf_valid\n",
        "\n",
        "    # is there a way of spling x,y without having to load the data and then delete\n",
        "    train_ds = lgb.Dataset(data=xtrain,label=ytrain,feature_name=fcolx)\n",
        "    valid_ds = lgb.Dataset(data=xvalid,label=yvalid,feature_name=fcolx)\n",
        "\n",
        "    keep_training_booster_val = True\n",
        "    booster = None\n",
        "\n",
        "   # print(f'INTITIAL############')\n",
        "    initial_scores = gbm.predict(xtrain) #####INITIAL SCORES\n",
        "    #derror = Rmetrics(ytrain, initial_scores)\n",
        "    #pprint(derror)\n",
        "    #gbm.add_valid(train_ds, initial_scores)\n",
        "    #gbm.continue_training(init_scores=initial_scores)\n",
        "    train_ds.set_init_score(initial_scores)\n",
        "    gbm = lgb.train(params,\n",
        "                train_ds,\n",
        "                num_round,\n",
        "                valid_sets=valid_ds,\n",
        "                feval=mean_error,\n",
        "                #init_model=booster,\n",
        "                keep_training_booster=True, # lear what it does\n",
        "                init_model=gbm,\n",
        "                callbacks=[lgb.early_stopping(stopping_rounds=50)],\n",
        "                )\n",
        "\n",
        "    gbm.save_model(modelpath_i1)\n",
        "    gbm.save_model(modelpath_i1_best,num_iteration=gbm.best_iteration)\n",
        "\n",
        "out_val_pd = pd.DataFrame(out_val,columns=['files'])\n",
        "out_val_pd.to_csv(os.path.join(save_dir,'validation_files.csv'),index=None)\n",
        "\n",
        "pvalid = gbm.predict(xvalid)\n",
        "derrorf = Rmetrics(yvalid, pvalid)\n",
        "derrorf.to_csv(os.path.join(save_dir,'metrics_f1.csv'), index=False)\n",
        "pprint(derror)\n",
        "pprint(derrorf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCnPvrCXxsnn"
      },
      "outputs": [],
      "source": [
        "# now its more like it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-ERWGKtxsnn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFWgfil6xsnn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7_29vVaxsnn"
      },
      "source": [
        "##### Assume you already have your existing model and data\n",
        "existing_model = lgb.Booster(model_file='existing_model.txt')\n",
        "new_data = np.random.rand(100, 10)  # Example new data, replace with your actual new data\n",
        "\n",
        "##### Assume you have initial scores for the new data\n",
        "initial_scores = np.random.rand(100)\n",
        "\n",
        "##### Perform continued training with the new data and initial scores\n",
        "existing_model.add_valid(new_data, initial_scores)\n",
        "existing_model.continue_training(init_scores=initial_scores)\n",
        "\n",
        "##### Save the updated model\n",
        "existing_model.save_model('updated_model.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3df_sg_gxsno"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHOdajBXxsnp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "envml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}